{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-vision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zs9LpZfDPqw",
        "outputId": "cf392630-4501-4602-a136-e8517cd80d80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (3.10.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract opencv-python-headless pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxkF5129Dgmm",
        "outputId": "72adf7f0-08ce-4d87-890b-3ff913739a37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Collecting pytesseract\n",
            "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Using cached pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mCrPW-HRAix-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import io\n",
        "from google.cloud import vision\n",
        "from google.oauth2 import service_account\n",
        "from PIL import Image\n",
        "\n",
        "# Authenticate to Google Cloud Vision API\n",
        "KEY_PATH = \"/content/glassy-azimuth-463621-t4-b9f25b9f7b00.json\"\n",
        "credentials = service_account.Credentials.from_service_account_file(KEY_PATH)\n",
        "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
        "\n",
        "def process_image(image_path):\n",
        "    # Run OCR on the image using Google Vision\n",
        "    with io.open(image_path, \"rb\") as image_file:\n",
        "        content = image_file.read()\n",
        "    image = vision.Image(content=content)\n",
        "    response = client.document_text_detection(image=image)\n",
        "    return response.full_text_annotation\n",
        "\n",
        "def extract_blocks(document):\n",
        "    # Extract text blocks and bounding box coordinates\n",
        "    blocks = []\n",
        "    for page in document.pages:\n",
        "        for block in page.blocks:\n",
        "            x0 = min(v.x for v in block.bounding_box.vertices)\n",
        "            x1 = max(v.x for v in block.bounding_box.vertices)\n",
        "            y0 = min(v.y for v in block.bounding_box.vertices)\n",
        "            y1 = max(v.y for v in block.bounding_box.vertices)\n",
        "            text = \"\"\n",
        "            for para in block.paragraphs:\n",
        "                for word in para.words:\n",
        "                    text += \"\".join([s.text for s in word.symbols]) + \" \"\n",
        "            blocks.append({\"text\": text.strip(), \"x0\": x0, \"x1\": x1, \"y0\": y0, \"y1\": y1})\n",
        "    return blocks\n",
        "\n",
        "def stitch_blocks(blocks):\n",
        "    # Combine text from blocks by vertical position\n",
        "    blocks_sorted = sorted(blocks, key=lambda b: (b[\"y0\"], b[\"x0\"]))\n",
        "    return \"\\n\".join([b[\"text\"] for b in blocks_sorted])\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize whitespace and remove trailing punctuation\n",
        "    if not text:\n",
        "        return None\n",
        "    text = text.replace('\\u2800', ' ').replace('\\xad', '')\n",
        "    text = re.sub(r'[-–]+\\s+', '', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = re.sub(r'[.,;]*$', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "all_residents = []\n",
        "all_deceased = []\n",
        "\n",
        "image_files = [f\"{n}.jpeg\" for n in range(104, 109)]\n",
        "\n",
        "for image_file in image_files:\n",
        "    document = process_image(image_file)\n",
        "    blocks = extract_blocks(document)\n",
        "\n",
        "    # Split into left/right columns\n",
        "    column_threshold = 1100\n",
        "    left_column_blocks = [b for b in blocks if b[\"x0\"] < column_threshold]\n",
        "    right_column_blocks = [b for b in blocks if b[\"x0\"] >= column_threshold]\n",
        "    stitched_text = stitch_blocks(left_column_blocks) + \"\\n\" + stitch_blocks(right_column_blocks)\n",
        "\n",
        "    entries_raw = re.split(r'(?<=\\.)\\s+(?=[A-Z\"“])', stitched_text)\n",
        "\n",
        "    resident_entries = []\n",
        "    dead_entries = []\n",
        "    prev_last_name = None\n",
        "    prev_address = {\"Raw\": None, \"Indicator\": None}\n",
        "    ad_keywords = ['mortgage', 'repairing', 'store', 'per cent', 'trunks', 'wagons', 'tel']\n",
        "    honorifics = {\"mr\", \"mrs\", \"miss\", \"ms\", \"dr\"}\n",
        "    page_number = int(re.search(r'\\d+', image_file).group())\n",
        "    directory_name = \"Minneapolis 1900\"\n",
        "\n",
        "    for entry in entries_raw:\n",
        "        entry = entry.strip().replace('“', '\"').replace('”', '\"')\n",
        "        if not entry or any(k in entry.lower() for k in ad_keywords):\n",
        "            continue\n",
        "\n",
        "        # Match deceased entries with \"died\" and extract name/date/age\n",
        "        if \"died\" in entry.lower():\n",
        "            death_match = re.match(r'^(\")?\\s*([^\\n,]+),\\s*died\\s+(.*?)(?:,|\\s)age[\\s:]*([0-9]{1,3})', entry, re.IGNORECASE)\n",
        "            if death_match:\n",
        "                quoted = death_match.group(1) == '\"'\n",
        "                raw_name = death_match.group(2).strip()\n",
        "                date_str = death_match.group(3).strip().rstrip('.,;')\n",
        "                age = death_match.group(4).strip()\n",
        "\n",
        "                # Name parsing\n",
        "                name_parts = [p for p in raw_name.split() if p.lower() not in honorifics]\n",
        "                first = middle = last = None\n",
        "                if len(name_parts) == 1:\n",
        "                    first = name_parts[0]\n",
        "                    last = prev_last_name\n",
        "                elif len(name_parts) == 2:\n",
        "                    if len(name_parts[1]) == 1:\n",
        "                        first, middle = name_parts\n",
        "                        last = prev_last_name\n",
        "                    else:\n",
        "                        last, first = name_parts\n",
        "                elif len(name_parts) >= 3:\n",
        "                    last, first = name_parts[:2]\n",
        "                    if len(name_parts[2]) == 1:\n",
        "                        middle = name_parts[2]\n",
        "                if quoted and not last:\n",
        "                    last = prev_last_name\n",
        "                if last:\n",
        "                    prev_last_name = last\n",
        "                dead_entries.append({\n",
        "                    \"FirstName\": first,\n",
        "                    \"MiddleInitial\": middle,\n",
        "                    \"LastName\": last,\n",
        "                    \"DateOfDeath\": date_str,\n",
        "                    \"Age\": age,\n",
        "                    \"DirectoryName\": directory_name,\n",
        "                    \"PageNumber\": page_number\n",
        "                })\n",
        "            continue\n",
        "\n",
        "        name_match = re.match(r'^(\")?\\s*([^\\n,]+),\\s*(.+)', entry)\n",
        "        if not name_match:\n",
        "            continue\n",
        "\n",
        "        quoted = name_match.group(1) == '\"'\n",
        "        raw_name_str = name_match.group(2).strip()\n",
        "        rest = name_match.group(3).strip()\n",
        "\n",
        "        # Parse moved entries: \"moved to <location>\"\n",
        "        move_match = re.match(r'(moved|removed) to\\s+(.*)', rest, re.IGNORECASE)\n",
        "        if move_match:\n",
        "            location = move_match.group(2).strip().rstrip('.')\n",
        "            name_parts = [p for p in raw_name_str.split() if p.lower() not in honorifics]\n",
        "            first = middle = last = None\n",
        "            if len(name_parts) == 1:\n",
        "                first = name_parts[0]\n",
        "                last = prev_last_name\n",
        "            elif len(name_parts) == 2:\n",
        "                if len(name_parts[1]) == 1:\n",
        "                    first, middle = name_parts\n",
        "                    last = prev_last_name\n",
        "                else:\n",
        "                    last, first = name_parts\n",
        "            elif len(name_parts) >= 3:\n",
        "                last, first = name_parts[:2]\n",
        "                if len(name_parts[2]) == 1:\n",
        "                    middle = name_parts[2]\n",
        "            if quoted and not last:\n",
        "                last = prev_last_name\n",
        "            if last:\n",
        "                prev_last_name = last\n",
        "            resident_entries.append({\n",
        "                \"FirstName\": first,\n",
        "                \"MiddleInitial\": middle,\n",
        "                \"LastName\": last,\n",
        "                \"Spouse\": None,\n",
        "                \"Occupation\": None,\n",
        "                \"CompanyName\": None,\n",
        "                \"HomeAddress\": {\n",
        "                    \"Raw\": location,\n",
        "                    \"Indicator\": \"moved\"\n",
        "                },\n",
        "                \"Telephone\": None,\n",
        "                \"DirectoryName\": directory_name,\n",
        "                \"PageNumber\": page_number\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        spouse = None\n",
        "        # Spouse name extraction if \"wid\" mentioned\n",
        "        paren_match = re.search(r'\\((.*?)\\)', raw_name_str)\n",
        "        if paren_match:\n",
        "            paren_content = paren_match.group(1).strip()\n",
        "            wid_match = re.match(r'wid\\s+(.*)', paren_content, re.IGNORECASE)\n",
        "            if wid_match:\n",
        "                spouse = wid_match.group(1).strip()\n",
        "            raw_name_str = re.sub(r'\\s*\\([^)]*\\)', '', raw_name_str).strip()\n",
        "\n",
        "        name_str = raw_name_str\n",
        "\n",
        "        # Skip \"see also\" cross-references\n",
        "        if re.match(r'see also\\b', rest.lower()):\n",
        "            prev_last_name = name_str\n",
        "            continue\n",
        "\n",
        "        # Extract address and indicator (r/b/rms)\n",
        "        addr_match = re.search(r'\\b(r|b|rms)\\b\\.?\\s*(?P<addr>[^.,\\n\"]+)', rest)\n",
        "        if not addr_match:\n",
        "            continue\n",
        "        indicator = addr_match.group(1)\n",
        "        address = addr_match.group(\"addr\").strip().rstrip('.')\n",
        "\n",
        "        # Track previous address if \"same\"\n",
        "        if address.lower() == \"same\":\n",
        "            address = prev_address[\"Raw\"]\n",
        "            indicator = prev_address[\"Indicator\"]\n",
        "        else:\n",
        "            prev_address = {\"Raw\": address, \"Indicator\": indicator}\n",
        "\n",
        "        # Extract telephone number if available\n",
        "        tel_match = re.search(r'\\btel\\.?\\s*([^.,;\\n]+)', rest, re.IGNORECASE)\n",
        "        telephone = tel_match.group(1).strip() if tel_match else None\n",
        "\n",
        "        # Parse occupation and company from pre-address text\n",
        "        pre_address = rest[:addr_match.start()].strip().rstrip(\",\")\n",
        "        tokens = pre_address.split()\n",
        "        occupation = tokens[0] if tokens else None\n",
        "        company = \" \".join(tokens[1:]) + \",\" if len(tokens) > 1 else \"\"\n",
        "\n",
        "        # Determine first, middle, last name from tokens\n",
        "        name_parts = [p for p in name_str.split() if p.lower() not in honorifics]\n",
        "        first = middle = last = None\n",
        "        if len(name_parts) == 1:\n",
        "            first = name_parts[0]\n",
        "            last = prev_last_name\n",
        "        elif len(name_parts) == 2:\n",
        "            if len(name_parts[1]) == 1:\n",
        "                first, middle = name_parts\n",
        "                last = prev_last_name\n",
        "            else:\n",
        "                last, first = name_parts\n",
        "        elif len(name_parts) >= 3:\n",
        "            last, first = name_parts[:2]\n",
        "            if len(name_parts[2]) == 1:\n",
        "                middle = name_parts[2]\n",
        "        if quoted and not last:\n",
        "            last = prev_last_name\n",
        "        if last:\n",
        "            prev_last_name = last\n",
        "\n",
        "        resident_entries.append({\n",
        "            \"FirstName\": first,\n",
        "            \"MiddleInitial\": middle,\n",
        "            \"LastName\": last,\n",
        "            \"Spouse\": spouse,\n",
        "            \"Occupation\": occupation,\n",
        "            \"CompanyName\": company,\n",
        "            \"HomeAddress\": {\n",
        "                \"Raw\": address,\n",
        "                \"Indicator\": indicator\n",
        "            },\n",
        "            \"Telephone\": telephone,\n",
        "            \"DirectoryName\": directory_name,\n",
        "            \"PageNumber\": page_number\n",
        "        })\n",
        "\n",
        "    # Post-processing: cleanup and split address fields\n",
        "    for entry in resident_entries:\n",
        "        entry[\"FirstName\"] = clean_text(entry.get(\"FirstName\"))\n",
        "        entry[\"MiddleInitial\"] = clean_text(entry.get(\"MiddleInitial\"))\n",
        "        entry[\"LastName\"] = clean_text(entry.get(\"LastName\"))\n",
        "        entry[\"Spouse\"] = clean_text(entry.get(\"Spouse\"))\n",
        "        entry[\"Occupation\"] = clean_text(entry.get(\"Occupation\"))\n",
        "        entry[\"CompanyName\"] = clean_text(entry.get(\"CompanyName\"))\n",
        "        entry[\"Telephone\"] = clean_text(entry.get(\"Telephone\"))\n",
        "        entry[\"HomeAddress\"][\"Raw\"] = clean_text(entry[\"HomeAddress\"][\"Raw\"])\n",
        "        raw_address = entry[\"HomeAddress\"].get(\"Raw\")\n",
        "        if raw_address:\n",
        "            parts = raw_address.split(maxsplit=1)\n",
        "            if parts and parts[0].isdigit():\n",
        "                entry[\"HomeAddress\"][\"StreetNumber\"] = parts[0]\n",
        "                entry[\"HomeAddress\"][\"StreetName\"] = parts[1] if len(parts) > 1 else None\n",
        "            else:\n",
        "                entry[\"HomeAddress\"][\"StreetNumber\"] = None\n",
        "                entry[\"HomeAddress\"][\"StreetName\"] = raw_address\n",
        "        del entry[\"HomeAddress\"][\"Raw\"]\n",
        "\n",
        "    all_residents.extend(resident_entries)\n",
        "    all_deceased.extend(dead_entries)\n",
        "\n",
        "with open(\"residents_combined.json\", \"w\") as f:\n",
        "    json.dump(all_residents, f, indent=2)\n",
        "\n",
        "with open(\"deceased_combined.json\", \"w\") as f:\n",
        "    json.dump(all_deceased, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in all_residents:\n",
        "    company = entry.get(\"CompanyName\")\n",
        "\n",
        "    work_address = None\n",
        "    if isinstance(company, str) and company.strip():\n",
        "        # If company name has a numeric part (e.g., address)\n",
        "        match = re.search(r'\\b(\\d+.*)', company)\n",
        "        if match:\n",
        "            work_address = match.group(1).strip()\n",
        "            company_cleaned = company[:match.start()].strip(\", \").strip()\n",
        "            entry[\"CompanyName\"] = company_cleaned if company_cleaned else None\n",
        "        else:\n",
        "            entry[\"CompanyName\"] = company.strip(\", \").strip() or None\n",
        "    else:\n",
        "        entry[\"CompanyName\"] = None\n",
        "\n",
        "    entry[\"WorkAddress\"] = work_address\n"
      ],
      "metadata": {
        "id": "CpU_6WPsPIcj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moved_residents = []\n",
        "\n",
        "for entry in all_residents:\n",
        "    if entry.get(\"HomeAddress\", {}).get(\"Indicator\") == \"moved\":\n",
        "        new_location = entry[\"HomeAddress\"].get(\"StreetName\") or \"\"\n",
        "        moved_entry = entry.copy()\n",
        "        moved_entry[\"NewLocation\"] = new_location\n",
        "        # Remove street info, keep only NewLocation\n",
        "        del moved_entry[\"HomeAddress\"]\n",
        "        moved_residents.append(moved_entry)\n",
        "\n",
        "# Remove moved entries from the main all_residents list\n",
        "all_residents = [\n",
        "    r for r in all_residents if r.get(\"HomeAddress\", {}).get(\"Indicator\") != \"moved\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "qLR7R6L3PJAL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"residents.json\", \"w\") as f:\n",
        "    json.dump(all_residents, f, indent=2)\n",
        "\n",
        "with open(\"deceased.json\", \"w\") as f:\n",
        "    json.dump(all_deceased, f, indent=2)\n",
        "\n",
        "with open(\"moved.json\", \"w\") as f:\n",
        "    json.dump(moved_residents, f, indent=2)\n"
      ],
      "metadata": {
        "id": "YbqhPc4UPj0l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6gJ77ai2P6Y9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}